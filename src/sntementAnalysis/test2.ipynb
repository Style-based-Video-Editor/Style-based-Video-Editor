{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.4.3-cp38-cp38-win_amd64.whl (7.1 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 171, in _merge_into_criterion\n",
      "    crit = self.state.criteria[name]\n",
      "KeyError: 'matplotlib'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\http\\client.py\", line 459, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\http\\client.py\", line 503, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 189, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 178, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 316, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 121, in resolve\n",
      "    self._result = resolver.resolve(\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 453, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 318, in resolve\n",
      "    name, crit = self._merge_into_criterion(r, parent=None)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 173, in _merge_into_criterion\n",
      "    crit = Criterion.from_requirement(self._p, requirement, parent)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 82, in from_requirement\n",
      "    if not cands:\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 124, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 143, in __bool__\n",
      "    return any(self)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 38, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 167, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 300, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 144, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 226, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 311, in _prepare_distribution\n",
      "    return self._factory.preparer.prepare_linked_requirement(\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 457, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 480, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 230, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 108, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 163, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 159, in iter\n",
      "    for x in it:\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 64, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\Users\\Noor AB\\Anaconda3\\envs\\sp\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n",
      "The system cannot find the path specified.\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000017155AE5DF0>: Failed to establish a new connection: [WinError 10053] An established connection was aborted by the software in your host machine')': /simple/tensorflow/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000017155B0F040>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed')': /simple/tensorflow/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000017155B0F1F0>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed')': /simple/tensorflow/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000017155B0F3A0>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed')': /simple/tensorflow/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000017155B0F550>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed')': /simple/tensorflow/\n",
      "ERROR: Could not find a version that satisfies the requirement tensorflow\n",
      "ERROR: No matching distribution found for tensorflow\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.4.3-cp38-cp38-win_amd64.whl (7.1 MB)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\noor ab\\anaconda3\\envs\\sp\\lib\\site-packages (from matplotlib) (1.21.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\noor ab\\anaconda3\\envs\\sp\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.4.0-cp38-cp38-win_amd64.whl (3.2 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\noor ab\\anaconda3\\envs\\sp\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.2-cp38-cp38-win_amd64.whl (52 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\noor ab\\anaconda3\\envs\\sp\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pillow, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 kiwisolver-1.3.2 matplotlib-3.4.3 pillow-8.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\NOORAB~1\\AppData\\Local\\Temp/ipykernel_6852/2971697587.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\NOORAB~1\\AppData\\Local\\Temp/ipykernel_6852/2702046137.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "\n",
    "str_punc = string.punctuation.replace(',', '').replace(\"'\",'')\n",
    "\n",
    "def clean(text):\n",
    "    global str_punc\n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "    text = text.lower()\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset & Preprocess text\n",
    "# dataset = pd.read_csv('../input/isear-emotion/isear_dataset.csv')\n",
    "df_train = pd.read_csv('../input/emotions-dataset-for-nlp/train.txt', names=['Text', 'Emotion'], sep=';')\n",
    "df_val = pd.read_csv('../input/emotions-dataset-for-nlp/val.txt', names=['Text', 'Emotion'], sep=';')\n",
    "df_test = pd.read_csv('../input/emotions-dataset-for-nlp/test.txt', names=['Text', 'Emotion'], sep=';')\n",
    "\n",
    "X_train = df_train['Text'].apply(clean)\n",
    "y_train = df_train['Emotion']\n",
    "\n",
    "X_test = df_test['Text'].apply(clean)\n",
    "y_test = df_test['Emotion']\n",
    "\n",
    "X_val = df_val['Text'].apply(clean)\n",
    "y_val = df_val['Emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classes counts\n",
    "colors = np.array([\"gray\", 'orangered', 'hotpink', 'gold', 'teal', 'cornflowerblue'])\n",
    "\n",
    "plt.bar(y_train.unique(),height=y_train.value_counts(), color=colors)\n",
    "plt.title(\"Training data - classes counts\")\n",
    "plt.show()\n",
    "\n",
    "plt.bar(y_test.unique(),height=y_test.value_counts(), color=colors)\n",
    "plt.title(\"Testing data - classes counts\")\n",
    "plt.show()\n",
    "\n",
    "plt.bar(y_val.unique(),height=y_val.value_counts(), color=colors)\n",
    "plt.title(\"Validation data - classes counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "y_val = le.transform(y_val)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_val = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(pd.concat([X_train, X_test], axis=0))\n",
    "\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "sequences_val = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "X_train = pad_sequences(sequences_train, maxlen=256, truncating='pre')\n",
    "X_test = pad_sequences(sequences_test, maxlen=256, truncating='pre')\n",
    "X_val = pad_sequences(sequences_val, maxlen=256, truncating='pre')\n",
    "\n",
    "vocabSize = len(tokenizer.index_word) + 1\n",
    "print(f\"Vocabulary size = {vocabSize}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read GloVE embeddings\n",
    "\n",
    "path_to_glove_file = '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\n",
    "num_tokens = vocabSize\n",
    "embedding_dim = 200\n",
    "hits = 0\n",
    "misses = 0\n",
    "embeddings_index = {}\n",
    "\n",
    "# Read word vectors\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "\n",
    "# Assign word vectors to our dictionary/vocabulary\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neural network architecture\n",
    "\n",
    "adam = Adam(learning_rate=0.005)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, 200, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(LSTM(256, dropout=0.2,recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2)))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1,\n",
    "                    batch_size=256,\n",
    "                    epochs=10,\n",
    "                    callbacks=[callback]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_val, y_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Loss & Accuracy\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify custom sample\n",
    "\n",
    "sentences = [\n",
    "            \"He's over the moon about being accepted to the university\",\n",
    "            \"Your point on this certain matter made me outrageous, how can you say so? This is insane.\",\n",
    "            \"I can't do it, I'm not ready to lose anything, just leave me alone\",\n",
    "            \"Merlin's beard harry, you can cast the Patronus charm! I'm amazed!\"\n",
    "            ]\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    sentence = clean(sentence)\n",
    "    sentence = tokenizer.texts_to_sequences([sentence])\n",
    "    sentence = pad_sequences(sentence, maxlen=256, truncating='pre')\n",
    "    result = le.inverse_transform(np.argmax(model.predict(sentence), axis=-1))[0]\n",
    "    proba =  np.max(model.predict(sentence))\n",
    "    print(f\"{result} : {proba}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "    \n",
    "with open('labelEncoder.pickle', 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "    \n",
    "    \n",
    "model.save('Emotion Recognition.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2b38e59c59fb6e5a47b73262d025a94199a3af81afb43eafe4bf51437c2f1e1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('sp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
