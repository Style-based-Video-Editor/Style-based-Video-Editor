{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7cWTp6-bTGv",
        "outputId": "b1e1c67e-4233-4ff6-b9aa-d0fc68bb414f"
      },
      "outputs": [],
      "source": [
        "!pip install gensim\n",
        "!pip install sklearn\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# add the full grams here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccRSiFIPc0K0",
        "outputId": "30035402-6231-48ef-9a79-36539cc279cf"
      },
      "outputs": [],
      "source": [
        "!unzip full_grams_sg_300_twitter.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1PJ0iSYdLB9"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from keras.models import load_model\n",
        "\n",
        "_dropout_rate = 0.2\n",
        "_dropout_rate_softmax = 0.5\n",
        "_number_of_inputs = 140 #max number of words /characters per doc(tweet)\n",
        "_vector_size = 300 #vector for each word\n",
        "_batch_size = 100\n",
        "_kernal_size= 5 #An integer or tuple/list of a single integer\n",
        "_pool_size = 3\n",
        "_noise_shape = (_batch_size,1,_number_of_inputs)\n",
        "_epochs = 25\n",
        "_test_size = 0.33 # percentage of test from the dataset\n",
        "_Learning_rate = 0.0001\n",
        "\n",
        "_aravec_model_name = \"full_grams_sg_300_twitter\" \n",
        "_emotional_modle_name = \"trail_rev.h5\"\n",
        "\n",
        "\n",
        "my_api_key = \"AIzaSyCUKEOsT6ecC3ods862vgsVOawWyii0NDQ\"\n",
        "my_cse_id = \"007967891901694126580:i3iq-cjlldq\"\n",
        "\n",
        "def google_search(search_term):\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
        "    res = service.cse().list(q=search_term, cx=my_cse_id).execute()\n",
        "    return res['spelling']['correctedQuery']\n",
        "  \n",
        "#text = input(\"enter arabic text\")\n",
        "\n",
        "#مسح التشكيل و علامات الترقيم و الحروف المتكررة---------\n",
        "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ»«•'''\n",
        "english_punctuations = string.punctuation\n",
        "english_numbers = \"0123456789\"\n",
        "punctuations_list = arabic_punctuations + english_punctuations + english_numbers\n",
        "\n",
        "arabic_diacritics = re.compile(\"\"\"\n",
        "                             ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                         \"\"\", re.VERBOSE)\n",
        "\n",
        "\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"إ\", \"ا\", text)\n",
        "    text = re.sub(\"أ\", \"ا\", text)\n",
        "    text = re.sub(\"آ\", \"ا\", text)\n",
        "    text = re.sub(\"ا\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_diacritics(text):\n",
        "    text = re.sub(arabic_diacritics, '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)\n",
        "\n",
        "\n",
        "def remove_repeating_char(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
        "#----------------------------------------------------\n",
        "\n",
        "#-------------tokenization and stop word removal\n",
        "def tokens_remove_stopwords(text):\n",
        "\n",
        "    text = text.split()\n",
        "    result = list()\n",
        "    ch = 0\n",
        "\n",
        "    arabic_stop_words = [\"من\", \"فى\", \"الي\", \"علي\", \"عن\", \"حتي\", \"مذ\", \"منذ\", \"و\", \"الا\", \"او\", \"ام\", \"ثم\", \"بل\", \"لكن\",\n",
        "                         \"كل\" , \"متى\" , \"يوم\"]\n",
        "\n",
        "    for word in text:\n",
        "        for stop_word in arabic_stop_words:\n",
        "            if word == stop_word:\n",
        "                ch = 1\n",
        "                break\n",
        "\n",
        "        if ch != 1:\n",
        "            result.append(word)\n",
        "\n",
        "        ch = 0\n",
        "\n",
        "    return result\n",
        "#_______________________________________\n",
        "\n",
        "#Rooting words\n",
        "def rooting(text):\n",
        "    result = list()\n",
        "    for word in text:\n",
        "        stemmer = ISRIStemmer()\n",
        "        result.append(stemmer.stem(word))\n",
        "    return result\n",
        "\n",
        "#remove english and empty strings\n",
        "def remove_english(tokens):\n",
        "    filtered_tokens = list()\n",
        "    for word in tokens:\n",
        "        if (not re.match(r'[a-zA-Z]+', word, re.I)) and word != '':\n",
        "            filtered_tokens.append(word)\n",
        "    return filtered_tokens\n",
        "\n",
        "\n",
        "def preprocess1(text):\n",
        "    text = str(text)\n",
        "    text = remove_diacritics(text)\n",
        "    text = remove_punctuations(text)\n",
        "    text = normalize_arabic(text)\n",
        "    text = remove_repeating_char(text)\n",
        "    tokens = re.split(\" \", text)\n",
        "    tokens = remove_english(tokens)\n",
        "    return tokens\n",
        "\n",
        "def preprocess2(text):\n",
        "    text = str(text)\n",
        "    text = remove_diacritics(text)\n",
        "    text = remove_punctuations(text)\n",
        "    text = normalize_arabic(text)\n",
        "    text = remove_repeating_char(text)\n",
        "    text = tokens_remove_stopwords(text)\n",
        "    text = remove_english(text)\n",
        "    text = rooting(text)\n",
        "    return text\n",
        "\n",
        "def embed_doc(text,t_model):\n",
        "    preprocessed_text = preprocess1(text)\n",
        "    #print(preprocessed_text)\n",
        "    \n",
        "    embedded_vectors = np.zeros(shape=(_number_of_inputs,_vector_size))#np array of arrays (array of 100/300 float number per word)\n",
        "    embedded_vectors_index = 0\n",
        "    for i in range(len(preprocessed_text)):\n",
        "        try:\n",
        "            embedded_vectors[embedded_vectors_index] = t_model.wv[preprocessed_text[i]]\n",
        "            embedded_vectors_index = embedded_vectors_index + 1\n",
        "        except:\n",
        "            try:\n",
        "                result = rooting([preprocessed_text[i]])[0]\n",
        "                embedded_vectors[embedded_vectors_index] = t_model.wv[result]\n",
        "                embedded_vectors_index = embedded_vectors_index + 1\n",
        "            except:\n",
        "                try:\n",
        "                    search_output = google_search(preprocessed_text[i])\n",
        "                    tokens = re.split(\" \", search_output)\n",
        "                    for j in range(len(tokens)):\n",
        "                        try:\n",
        "                            embedded_vectors[embedded_vectors_index] = t_model.wv[tokens[j]]\n",
        "                            embedded_vectors_index = embedded_vectors_index + 1\n",
        "                        except:\n",
        "                            pass #print(tokens[j] + \" Sub word cant be embedded\")\n",
        "                except:\n",
        "                     pass # print(preprocessed_text[i] + \"word cant be embedded\") #currently emojis can't be embedded and for any extreme case (skip wrongly written words)\n",
        "    return embedded_vectors\n",
        "  \n",
        "def embed_label(label):\n",
        "  if label == \"anger\":\n",
        "    return 0\n",
        "  if label == \"joy\":\n",
        "    return 1\n",
        "  if label == \"none\":\n",
        "    return 2\n",
        "  if label == \"surprise\":\n",
        "    return 3\n",
        "  if label == \"sadness\":\n",
        "    return 4\n",
        "  if label == \"fear\":\n",
        "    return 5\n",
        "  if label == \"sympathy\":\n",
        "    return 6\n",
        "  if label == \"love\":\n",
        "    return 7\n",
        "  \n",
        "def traslate_label(label):\n",
        "  if label == 0:\n",
        "    return  \"غضب\"\n",
        "  if label == 1:\n",
        "    return \"فرح\"\n",
        "  if label == 2:\n",
        "    return \"طبيعي\"\n",
        "  if label == 3:\n",
        "    return \"متفاجئ\"\n",
        "  if label == 4:\n",
        "    return \"حزن\"\n",
        "  if label == 5:\n",
        "    return \"خوف\"\n",
        "  if label == 6:\n",
        "    return \"تعاطف\"\n",
        "  if label == 7:\n",
        "    return \"حب\"\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY_OictarRGP"
      },
      "outputs": [],
      "source": [
        "def Demo(sentence):\n",
        "  t_model= gensim.models.Word2Vec.load(_aravec_model_name +'.mdl')\n",
        "  emotional_model = load_model(_emotional_modle_name)\n",
        "  doc=sentence\n",
        "  while(True):\n",
        "    if doc == 'break':\n",
        "      print(\"إلى اللقاء\")\n",
        "      break\n",
        "    else:\n",
        "      embedded_vector = embed_doc(doc,t_model)\n",
        "      shape= np.shape(embedded_vector)\n",
        "      embedded_vector = np.array(embedded_vector).reshape(1,shape[0],shape[1])\n",
        "      #label = emotional_model.predict_classes(embedded_vector)\n",
        "      predict_x=emotional_model.predict(embedded_vector) \n",
        "      classes_x=np.argmax(predict_x,axis=1)\n",
        "      textlabel = traslate_label(classes_x)\n",
        "      print (\"الحالة:\",textlabel)\n",
        "      \n",
        "      \n",
        "      \n",
        "Demo(\"انا حزين جدا\")          \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
